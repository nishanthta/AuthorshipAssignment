# -*- coding: utf-8 -*-
"""IR project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/0B84RrxRRiz6fbGhUUWgwR1B5ZFg0TWVCVUUxcFltSi1jVlB3
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.feature_extraction.text import CountVectorizer
import os, re
import numpy as np
from keras.utils import to_categorical
from keras.models import Sequential
from keras import layers
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

path  = "/content/drive/My Drive/IR_project/"

files = os.listdir(path)

features = [] #BoW features
text_data = []

for file in files:
  print(file)
  f = open(path + file)
  text = f.read()
  text_data.append(text)

vectorizer = CountVectorizer(min_df=0)
vectorizer.fit(text_data)
features = vectorizer.transform(text_data).toarray()
features = np.array(features)
features.shape

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(text_data)

vocab_size = len(tokenizer.word_index) + 1

X_train = tokenizer.texts_to_sequences(text_data)

maxlen = 20000

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)

path  = "/content/drive/My Drive/IR_test/"

files = os.listdir(path)

features = [] #BoW features
text_data = []

for file in files:
  print(file)
  f = open(path + file)
  text = f.read()
  text_data.append(text)

X_test = tokenizer.texts_to_sequences(text_data)

X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

y_test = np.array([0, 1, 2, 2])
y_test = to_categorical(y_test)
y_test

embedding_dim = 256

model = Sequential()

model.add(layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation='relu'))
model.add(layers.Dense(3, activation='softmax'))

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'],
              )

model.summary()

y_train = np.array([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2])
y_train = to_categorical(y_train)

history = model.fit(X_train, y_train,
                    epochs=5,
                    verbose=False, shuffle = True, validation_data = (X_test, y_test))

import matplotlib.pyplot as plt

acc = history.history['acc']
loss = history.history['loss']
acc = acc[:15]
loss = loss[:15]
epochs = range(1, len(acc) + 1)
#plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, loss, 'b', label='Training loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

X_train.shape

loss, accuracy = model.evaluate(X_train, y_train, verbose=False)

print("Training Accuracy: {:.4f}".format(accuracy))

model.predict(X_train)

y = model.predict(X_test)
idx = np.argmax(y, axis=-1)
y_pred = np.zeros( y.shape )
y_pred[ np.arange(y_pred.shape[0]), idx] = 1

y_pred

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test,y_pred))

def create_base_model(input_shape):
  model = Sequential()
  model.add(layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
  model.add(layers.Flatten())
  model.add(layers.Dense(10, activation='relu'))
  model.add(layers.Dense(5, activation='relu'))
  return model

input_shape = (20000,)
base_network = create_base_model(input_shape)
base_network.summary()

from keras.layers import Input, Subtract, Lambda, Dense
from keras.optimizers import RMSprop, Adam
from keras.models import Model
input_a = Input(shape = (input_shape))
input_b = Input(shape = (input_shape))

processed_a = base_network(input_a)
processed_b = base_network(input_b)

distance = Subtract()([processed_a, processed_b])

distance = Lambda(lambda x : np.absolute(x))(distance)

predictions = Dense(1, activation = 'sigmoid')(distance)

model = Model(inputs=[input_a, input_b], outputs=predictions)

rms = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08)

model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 1e-4), metrics = ['accuracy'])

print(model.summary())

'''
Create balanced training/test examples from the given datapoints
'''

x1_train, x2_train, y_train = [], [], []
for i in range(1,4):
  x1_train.append(X_train[0])
  x2_train.append(X_train[i])
  y_train.append(1) #similar documents
  
for i in range(2,4):
  x1_train.append(X_train[1])
  x2_train.append(X_train[i])
  y_train.append(1) #similar documents
  
for i in range(3,4):
  x1_train.append(X_train[2])
  x2_train.append(X_train[i])
  y_train.append(1) #similar documents
  
x1_train.append(X_train[4])
x2_train.append(X_train[5])
y_train.append(1)

x1_train.append(X_train[4])
x2_train.append(X_train[6])
y_train.append(1)

x1_train.append(X_train[5])
x2_train.append(X_train[6])
y_train.append(1)

for i in range(8,11):
  x1_train.append(X_train[7])
  x2_train.append(X_train[i])
  y_train.append(1) #similar documents
  
for i in range(9,11):
  x1_train.append(X_train[8])
  x2_train.append(X_train[i])
  y_train.append(1) #similar documents
  
for i in range(10,11):
  x1_train.append(X_train[9])
  x2_train.append(X_train[i])
  y_train.append(1) #similar documents
  
print(len(x1_train))
  
x1_train.append(X_train[0])
x2_train.append(X_train[4])
y_train.append(0)
x1_train.append(X_train[0])
x2_train.append(X_train[5])
y_train.append(0)
x1_train.append(X_train[5])
x2_train.append(X_train[7])
y_train.append(0)
x1_train.append(X_train[9])
x2_train.append(X_train[4])
y_train.append(0)
x1_train.append(X_train[2])
x2_train.append(X_train[5])
y_train.append(0)
x1_train.append(X_train[6])
x2_train.append(X_train[3])
y_train.append(0)
x1_train.append(X_train[5])
x2_train.append(X_train[10])
y_train.append(0)
x1_train.append(X_train[9])
x2_train.append(X_train[6])
y_train.append(0)
x1_train.append(X_train[1])
x2_train.append(X_train[4])
y_train.append(0)
x1_train.append(X_train[1])
x2_train.append(X_train[5])
y_train.append(0)
x1_train.append(X_train[4])
x2_train.append(X_train[7])
y_train.append(0)
x1_train.append(X_train[10])
x2_train.append(X_train[1])
y_train.append(0)
x1_train.append(X_train[6])
x2_train.append(X_train[0])
y_train.append(0)
x1_train.append(X_train[6])
x2_train.append(X_train[9])
y_train.append(0)
x1_train.append(X_train[8])
x2_train.append(X_train[3])
y_train.append(0)

x1_train, x2_train, y_train = np.array(x1_train), np.array(x2_train), np.array(y_train) 

'''
Fit the model on the produced datapoints
'''

history = model.fit([x1_train, x2_train], y_train, epochs = 30, shuffle = True)

acc = history.history['acc']
loss = history.history['loss']
acc = acc[:15]
loss = loss[:15]
epochs = range(1, len(acc) + 1)
#plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, loss, 'b', label='Training loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

model.predict(X_test)

loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['acc']
    #val_acc = history.history['val_acc']
    loss = history.history['loss']
    #val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    #plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    #plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training loss')
    plt.legend()

plot_history(history)

def create_embedding_matrix(filepath, word_index, embedding_dim):
    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    with open(filepath, encoding="utf8") as f:
        for line in f:
            word, *vector = line.split()
            if word in word_index:
                idx = word_index[word] 
                embedding_matrix[idx] = np.array(
                    vector, dtype=np.float32)[:embedding_dim]

    return embedding_matrix

embedding_dim = 50
embedding_matrix = create_embedding_matrix('data/glove/glove.6B.50d.txt',tokenizer.word_index, embedding_dim)

nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))

nonzero_elements / vocab_size

embedding_dim = 50

model = Sequential()
model.add(layers.Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length=maxlen))
model.add(layers.Conv1D(32, 5, activation='relu'))
model.add(layers.GlobalMaxPooling1D())
model.add(layers.Dense(10, activation='relu'))
model.add(layers.Dense(3, activation='softmax'))
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train,
                    epochs=50,
                    verbose=False,
                    validation_data=(X_test, y_test))
loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))
plot_history(history)

model.predict(X_test)

y = model.predict(X_test)
idx = np.argmax(y, axis=-1)
y_pred = np.zeros( y.shape )
y_pred[ np.arange(y_pred.shape[0]), idx] = 1

print(classification_report(y_test,y_pred))

model.predict(X_train)

